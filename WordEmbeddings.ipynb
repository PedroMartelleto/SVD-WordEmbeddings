{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pedro Martelleto e João Marcos Cardoso da Silva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings com a fatoração da matriz de Informação Mútua\n",
    "\n",
    "Nos últimos anos, o campo de Processamento de Linguagem natural com o Aprendizado de Máquina tem se desenvolvido rapidamente. Os novos modelos são capazes de aprender padrões muito mais complexos que os métodos clássicos do campo, que dependiam de features construídas à mão e sequências de operações bem definidas pelo programador.\n",
    "\n",
    "Porém, para o bom funcionamento desses modelos, especialmente os de Deep Learning, como as Redes Neurais Recorrentes e os Transformers, é necessário que os textos sejam de alguma forma convertidos do seu espaço discreto a vetores densos. Dessa forma, permite-se o processamento dessas palavras por redes neurais, as quais trabalham por meio de operações aprendidas em espaços vetoriais densos de alta dimensão.\n",
    "\n",
    "Para esse propósito, é muito comum na literatura do campo o uso da técnica do word2vec, que por meio de um processo iterativo de otimização cria um vetor denso associado a cada palavra, contendo em suas muitas dimensões o significado correspondente àquele termo. Contudo, como foi mostrado [nesse artigo](https://papers.nips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html), o word2vec é equivalente a uma fatoração da matriz de Informação Mútua Pontual, fato que é, em grande parte, responsável pelo seu funcionamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Informação Mútua Pontual\n",
    "\n",
    "A utilização da informação mútua pontual entre palavras para a construção do embedding é motivada pela chamada Hipótese Distribucional, que diz que o significado de uma palavra é definido pela distribuição das palavras contidas ao seu redor. Esse fato é resumido pela frase \"a word is characterized by the company it keeps\", de John Rupert Firth. Por esse motivo, é motivada a utilização dessa métrica para a construção dos embeddings, a qual é descrita a seguir.\n",
    "\n",
    "Assim, sejam W e C duas palavras distintas. Suponha que as suas frequências no corpus analisado são, respectivamente, P(W) e P(C). Seja também definida a probabilidade P(W, C), que corresponde à frequência na qual ambas as palavras ocorrem em um mesmo contexto (por exemplo, na mesma frase ou dentro de uma janela de 5 palavras).\n",
    "\n",
    "Fica então definida a Informação Mútua Pontual (PMI) entre W e C:\n",
    "\n",
    "$$\\text{PMI}(W, C) = \\log{\\left[\\frac{P(W,C)}{P(W)P(C)}\\right]}$$\n",
    "\n",
    "Essa medida tem as seguintes propriedades:\n",
    " \n",
    " - PMI(W, C) = 0 se e somente se as palavras W e C são independentes entre si (ou seja, a ocorrência de uma não afeta a chance da segunda ocorrer).\n",
    " - PMI(W, C) > 0 se e somente se a observação de W aumenta a probabilidade de observarmos C, ou vice-versa. Quanto maior esse valor, mais frequentemente as duas palavras aparecem no mesmo contexto.\n",
    " - PMI(W, C) < 0 se e somente se a observação de W diminui a probabilidade de observarmos C, ou vice-versa. Quanto menor esse número, mais raramente as duas palavras aparecem no mesmo contexto.\n",
    " \n",
    "No sentido da Teoria da Informação, a função PMI mede exatamente a Quantidade de Informação (em bits, caso o logaritmo seja na base 2) que uma observação a respeito de W nos dá sobre a distribuição de C, ou vice-versa.\n",
    "\n",
    "Como os valores de P(W), P(C) e principalmente P(W,C) não são conhecidos, mas sim estimados a partir de um conjunto de dados finito, valores negativos para a PMI (que indicam palavras que ocorrem pouco) são frequentemente mais ruidosos que os valores positivos. Por esse motivo, dentre outros, é mais comum na literatura utilizar a função PPMI, que substitui valores negativos por 0:\n",
    "\n",
    "$$\\text{PPMI}(W, C) = \\max(0, \\text{PMI}(W, C))$$\n",
    "\n",
    "Assim, podemos construir a chamada matriz PPMI, de dimensão N por N, onde N é o número de palavras no vocabulário estudado, e cada uma das suas entradas, de índices (i,j) contém o valor relativo à $\\text{PPMI}(W_i, W_j)$, onde $W_i$ e $W_j$ são as palavras correspondentes à linha $i$ e coluna $j$ da matriz. Essa matriz contém então informação a respeito da distribuição de todos os pares de palavras no conjunto de dados, podendo ser utilizada para a geração de embeddings para os termos que representem bem os significados dos mesmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Fatoração Aproximada\n",
    "\n",
    "A matriz PPMI, apesar de muitas vantagens teóricas, é extremamente grande. De fato, ela possui um número de elementos igual ao quadrado do tamanho do vocabulário. Logo, não é ideal trabalhar com a mesma, tanto por motivos de eficiência computacional, uso de memória e mesmo precisão das estimações feitas a partir do dataset finito. Além disso, podemos pensar nesse problema em termos de tamanho da matriz na memória. Se temos 10^6 palavras no vocabulário (o que não é incomum), teremos 10^12 entradas na matriz PPMI. Isto é, assumindo o uso de float32, a matriz terá tamanho de $2 * 10^12$ bytes (2 terabytes). Não é necessário muito detalhamento para entender que esse fato torna inviável a utilização da PPMI sem nenhuma outra medida.\n",
    "\n",
    "Para resolver esse problema, fazemos duas mudanças. A primeira é fatorar a matriz para que ela tenha um rank menor. Isso aumentará a eficiência dos nossos embeddings (guardados por menos números), diminuindo a esparsidade, e mantendo somente as características mais importantes:\n",
    "\n",
    "$$M_\\text{PPMI} \\approx W \\cdot V^T$$\n",
    "\n",
    "$W$ e $V$ são matrizes $N$ por $d$, com $d < N$.\n",
    "\n",
    "A segunda medida adotada é usar uma representação esparsa da matriz na memória, otimizando o número grande de 0's presentes na matriz.\n",
    "\n",
    "É conhecido da literatura que métodos iterativos como o word2vec e o GloVe estão implicitamente realizando uma fatoração análoga a esta, com os embeddings finais sendo as linhas da matriz $W$ (embora não na matriz PPMI, mas em uma relacionada). Contudo, nesse trabalho, faremos essa fatoração diretamente a partir da matriz, utilizando o SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "\n",
    "A técnica SVD é uma generalização da decomposição em autovetores e autovalores que também pode ser extendida a matriz singulares ou não quadradas. No nosso caso, a matriz $M_{PPMI}$ é fatorada da seguinte forma:\n",
    "\n",
    "$$M_\\text{PPMI} = U \\Sigma V^T$$\n",
    "\n",
    "$U$ e $V$ são matrizes ortogonais e $\\Sigma$ é uma matriz diagonal contendo os Valores Singulares.\n",
    "\n",
    "É possível usar o SVD para criar uma fatoração aproximada da matriz em rank menor, se usarmos somende os $d$ maiores valores singulares e as respectivas linhas e colunas nas matrizes ortogonais:\n",
    "\n",
    "$$M_\\text{PPMI} = U' \\Sigma' V'^T$$\n",
    "\n",
    "$\\Sigma'$ é uma matriz quadrada $d$ por $d$ e $U'$ e $V'$ são matrizes $N$ por $d$, como descrito acima.\n",
    "\n",
    "Assim, se definirmos $W = U' \\Sigma'$, recuperamos a matriz dos embeddings como descrito na seção anterior.\n",
    "\n",
    "Por motivos computacionais, já que a matriz PPMI é armazenada de forma esparsa, foi utilizada uma variação do algoritmo do SVD que calcula uma aproximação estocástica dessa fatoração."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística\n",
    "\n",
    "Como um exemplo da aplicação desses embeddings, iremos treinar um modelo de regressão logística para detectar o sentimento a partir de um tweet em português (se Positivo ou Negativo).\n",
    "\n",
    "Para isso, criamos um único vetor $x$ para representar cada tweet, o qual é composto pela média dos embeddings de cada palavra que compõe o texto. Como as dimensões dos vetores possuem significado semântico e estão no mesmo espaço vetorial, a sua média pode, de certa forma, representar o conteúdo da frase.\n",
    "\n",
    "Em seguida, definimos um vetor de pesos $\\textbf{w}$, cujos parâmetros são considerados treináveis. Ou seja, serão otimizados para o nosso problema. Seja $X$ a matriz cujas linhas são os vetores $x$ para cada tweet do dataset. Temos então a seguinte função, que faz as predições dos valores de $y$ a partir dos parâmetros:\n",
    "\n",
    "$$\\hat{Y} = \\sigma(X \\textbf{w})$$\n",
    "\n",
    "onde função logística $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ é aplicada elemento a elemento. A função logística tem a propriedade de que todos os valores da sua saída estão contidos entre 0 e 1.\n",
    "\n",
    "Assim, definimos o valor \"esperado\" de $Y$ como sendo 1 para frases de sentimento \"Positivo\" e 0 para frases de sentimento \"Negativo\". Nosso objetivo então é encontrar os valores de $w$ que levem a predições $\\hat{Y}$ próximas do valor esperado $Y$. Podemos fazer isso minimizando a seguinte função de custo, chamada de entropia cruzada binária (sua definição e motivação vêm da Teoria da Informação, mas para os propósitos desse trabalho basta dizer que ela é minimizada quanto o preditor possui 100% de acurácia):\n",
    "\n",
    "$$L(\\textbf{w}) = - \\left[ Y \\cdot (\\log \\hat{Y}) + (1- Y) \\cdot (\\log(1 - \\hat{Y}))\\right]$$\n",
    "\n",
    "Na equação acima, subtrações e a aplicação logaritmo são feitas elemento a elemento e as multiplicações são produtos escalares.\n",
    "\n",
    "### Otimização\n",
    "\n",
    "Podemos encontrar os melhores valores para $\\textbf{w}$ utilizando o método do Gradiente Descendente, que consiste em fazer a seguinte atualização repetidas vezes, onde $\\alpha$ é um número real pequeno chamado de taxa de aprendizado:\n",
    "\n",
    "$$\\textbf{w} \\leftarrow \\textbf{w} - \\alpha [\\nabla L(\\textbf{w})]^T$$\n",
    "\n",
    "Assim, é necessário calcular o gradiente (aqui usamos vetores linha como gradientes) da função de custo em função de $\\textbf{w}$:\n",
    "\n",
    "$$\\nabla L(\\textbf{w}) = \\left(\\hat{Y}^T - Y^T\\right)X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports usados no código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install datasets\n",
    "!pip3 install datasets unidecode ipywidgets tqdm --quiet\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import scipy.sparse\n",
    "import scipy\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from cb91visuals import *\n",
    "\n",
    "dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset capes (/home/Pedro/.cache/huggingface/datasets/capes/en-pt/1.0.0/b2c08eed67d23b54883a184eb830ecc54bc1cb7f7e4527ee45586ef5bbd0cc8f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da268494b6e4027a47e49f70411b8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'en': 'in this thesis we present two distinct research lines, namely, the first, referring to chapters 2 and 3, apply statistical techniques to the analysis of synthetic aperture radar (sar) images, and the second, referring to chapter 4, we examined problems concerning parameter estimation by maximum likelihood in exponential-poisson distribution.',\n",
       "  'pt': 'nessa tese apresentamos duas linhas de pesquisa distintas, a saber, na primeira, referente aos capítulos 1 e 3 aplicamos técnicas estatísticas à análise de imagens do satélite de abertura sintética (sar) e, na segunda, referente ao capítulo 2, examinamos problemas relativos à estimação de parâmetros por máxima verossimilhança na distribuição exponencial-poisson.'},\n",
       " {'en': 'in chapter 2 we study the problem of estimating edges in sar images, which by nature are fraught with multiplicative noise, also known as speckle.',\n",
       "  'pt': 'no capítulo 1 estudamos o problema de estimação de bordas em imagens sar.'},\n",
       " {'en': 'this noise leads to quite unsatisfactory results when conventional methods of image analysis are applied.',\n",
       "  'pt': 'tais imagens são repletas de ruído do tipo multiplicativo, também conhecido como salpico, o que torna os resultados apresentados pelos métodos convencionais de análise de imagens bastante insatisfatórios.'},\n",
       " {'en': 'it contaminates the image in a way that cannot be simply subtracted from the image, making the edge location estimation methods not effective, confusing noise with different regions edges.',\n",
       "  'pt': 'o ruído, por inserir uma contaminação que não pode ser simplesmente subtraída da imagem, faz com que os métodos de estimação da localização de bordas não sejam eficazes, frequentemente confundindo o ruído com bordas entre regiões distintas.'},\n",
       " {'en': 'by the other hand, a wrong point estimate is useless, and this is a problem.',\n",
       "  'pt': 'uma estimativa pontual errada da borda é algo que para nada serve, o que constitui um problema.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('capes')\n",
    "dataset['train']['translation'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aqui, fazemos o processamento do dataset para construir a PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 1157610\n",
      "})}\n",
      "Pre-processing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157610/1157610 [01:42<00:00, 11341.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word map...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157610/1157610 [01:25<00:00, 13610.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ppmi matrix from disk...\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset)\n",
    "\n",
    "texts = []\n",
    "\n",
    "# Removes punctuation and numbers from the text\n",
    "print(\"Pre-processing dataset...\")\n",
    "def process_text(text):\n",
    "  txt = unidecode(text).replace('\\n' , '') # Normalizes string\n",
    "  txt = re.sub(r\"\\([^()]*\\)\", \"\", txt) # Removes everything inside parenthesis\n",
    "  txt = re.sub(r'\\w*\\d\\w*', ' ', txt).strip() # Remove numbers\n",
    "  txt = re.sub(r'[^a-zA-Z0-9]', ' ', txt) # Removes all non-alphanumeric characters\n",
    "  txt = re.sub(r\"\\s{2,}\", ' ', txt).strip() # Removes double or more spaces\n",
    "  return txt\n",
    "\n",
    "# Processes the dataset\n",
    "for text in tqdm(dataset['train']['translation']):\n",
    "  texts.append(process_text(text['pt']))\n",
    "\n",
    "MAX_WORD_SIZE = 28\n",
    "MIN_FREQ = 70\n",
    "\n",
    "# Helper function that iterates over the dataset\n",
    "def iterate_dataset(window_size, word_fn, *args):\n",
    "  for text in tqdm(texts):\n",
    "    words = text.split(\" \")\n",
    "    for i in range(len(words)):\n",
    "      word = words[i]\n",
    "\n",
    "      if len(word) > MAX_WORD_SIZE:\n",
    "        continue\n",
    "\n",
    "      for j in range(-window_size, window_size):\n",
    "        if j + i >= 0 and j + i < len(words):\n",
    "          if not word_fn is None and len(words[j+i]) <= MAX_WORD_SIZE:\n",
    "            word_fn(word, words[j + i], *args)\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "# Counts the frequency of each word\n",
    "def word_map_create_fn(word, context, words_map, words_count):\n",
    "  if not word in words_map:\n",
    "    words_map[word] = len(words_map)\n",
    "    words_count[word] = 0\n",
    "  \n",
    "  words_count[word] += 1\n",
    "\n",
    "words_map = {}\n",
    "words_count = {}\n",
    "\n",
    "# First, we iterate over the dataset to assign each word to an index\n",
    "print(\"Creating word map...\")\n",
    "iterate_dataset(window_size,\n",
    "                word_map_create_fn,\n",
    "                words_map, words_count)\n",
    "\n",
    "# Loads cache if the file exists\n",
    "if os.path.exists('capes_ppmi.npz'):\n",
    "  print(\"Loading ppmi matrix from disk...\")\n",
    "  sparse_ppmi = scipy.sparse.load_npz('capes_ppmi.npz')\n",
    "# Otherwise, we compute the PPMI matrix\n",
    "else:\n",
    "  # Counts the frequency of each word in context\n",
    "  def freq_ctx_fn(word, context, words_map, freq_matrix):\n",
    "    if word in words_map and context in words_map:\n",
    "      freq_matrix[words_map[word], words_map[context]] += 1\n",
    "\n",
    "  # Creates the frequency matrix\n",
    "  def create_freq_matrix(window_size):\n",
    "    words_map = {}\n",
    "    words_count = {}\n",
    "\n",
    "    # First, we iterate over the dataset to assign each word to an index\n",
    "    print(\"Creating word map...\")\n",
    "    iterate_dataset(window_size,\n",
    "                    word_map_create_fn,\n",
    "                    words_map, words_count)\n",
    "    \n",
    "    # Remove words that are not frequent enough\n",
    "    for word, freq in words_count.items():\n",
    "      if freq < MIN_FREQ:\n",
    "        if word in words_map:\n",
    "          del words_map[word]\n",
    "\n",
    "    # Adjusts the word map indices\n",
    "    words = []\n",
    "    i = 0\n",
    "    for w in words_map:\n",
    "      words_map[w] = i\n",
    "      words.append(w)\n",
    "      i += 1\n",
    "\n",
    "    print(\"Vocab size:\", len(words_map))\n",
    "    \n",
    "    # Creates the frequency matrix\n",
    "    print(\"Allocating frequency matrix: {:.2f} GB\".format((len(words_map) * len(words_map)) * 2 / (1000 ** 3)))\n",
    "    freq_matrix = np.zeros((len(words_map), len(words_map)), dtype=np.float32)\n",
    "\n",
    "    print(\"Creating frequency matrix...\")\n",
    "    iterate_dataset(window_size, freq_ctx_fn, words_map, freq_matrix)\n",
    "    return freq_matrix, words\n",
    "\n",
    "  freq_matrix, words = create_freq_matrix(window_size=2)\n",
    "  \n",
    "  # Normalizes the frequency matrix and computes log information\n",
    "  def create_ppmi_matrix(freq_matrix, positive=True):\n",
    "    total = freq_matrix.sum()\n",
    "    word_counts = np.sum(freq_matrix, axis=1)\n",
    "    context_counts = np.sum(freq_matrix, axis=0)\n",
    "\n",
    "    expected = np.outer(word_counts, context_counts) / total\n",
    "    freq_matrix = freq_matrix / expected\n",
    "\n",
    "    with np.errstate(divide='ignore'):\n",
    "      freq_matrix = np.log(freq_matrix)\n",
    "\n",
    "    freq_matrix[np.isinf(freq_matrix)] = 0.0\n",
    "\n",
    "    if positive:\n",
    "      freq_matrix[freq_matrix < 0] = 0.0\n",
    "\n",
    "    return freq_matrix\n",
    "\n",
    "  ppmi = create_ppmi_matrix(freq_matrix)\n",
    "  sparse_ppmi = scipy.sparse.coo_matrix(ppmi)\n",
    "  scipy.sparse.save_npz('capes_ppmi.npz', sparse_ppmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Randomizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "# Computes the randomized SVD of the PPMI matrix (useful for sparse matrices)\n",
    "U, Sigma, VT = randomized_svd(sparse_ppmi, \n",
    "                              n_components=dim,\n",
    "                              n_iter=8,\n",
    "                              random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcula os embeddings e define funções para testá-los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the word embeddings\n",
    "W = np.dot(U, np.diag(np.sqrt(Sigma)))\n",
    "\n",
    "# Helper functions\n",
    "def get_embedding(word):\n",
    "    if word in words_map:\n",
    "        return W[words_map[word],]\n",
    "    else:\n",
    "        return np.zeros(dim)\n",
    "\n",
    "def get_embeddings(words):\n",
    "    return np.array([get_embedding(word) for word in words])\n",
    "\n",
    "def get_closest_words(embedding, count):\n",
    "    distances = np.linalg.norm(W - embedding, axis=1)\n",
    "    return np.asarray(words, dtype=np.str)[np.argsort(distances)[:count]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot de proximidade dos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Pedro/.local/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:501: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examples of word embeddings similarity\n",
    "\n",
    "words_to_plot = [ 'azul', 'amarelo', 'branco', 'cultura', 'arte', 'educacao', 'musica', 'teatro',\n",
    "                  'cinema', 'verde', 'rosa', 'integral', 'derivada', 'vetor', \n",
    "                  'series', 'poisson']\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(get_embeddings(words_to_plot))\n",
    "plt.scatter(x=X[:,0], y=X[:,1])\n",
    " \n",
    "# Inserts labels to the plot\n",
    "for i in range(len(words_to_plot)):\n",
    "    plt.text(X[i, 0], X[i, 1] + 0.05, words_to_plot[i], ha='center', va='center', fontsize = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palavras mais próximas à palavra dada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds words closest to the given word\n",
    "get_closest_words(get_embedding('amor'), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento do dataset de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Processes tweet dataset\n",
    "\n",
    "df = pd.read_csv('NoThemeTweets.csv')\n",
    "\n",
    "df = df[['tweet_text', 'sentiment']]\n",
    "df['sentiment'] = (df['sentiment'] == 'Positivo').astype(int)\n",
    "\n",
    "def process_tweet(text):\n",
    "    txt = unidecode(text).replace('\\n' , '').lower() # Normalizes string\n",
    "    txt = re.sub(r\"@\\w+\", \"\", txt)\n",
    "    txt = re.sub(r\"\\([^()]*\\)\", \"\", txt) # Removes everything inside parenthesis\n",
    "    txt = re.sub(r'\\w*\\d\\w*', ' ', txt).strip() # Remove numbers\n",
    "    txt = re.sub(r'[^a-zA-Z0-9]', ' ', txt) # Removes all non-alphanumeric characters\n",
    "    txt = re.sub(r\"\\s{2,}\", ' ', txt).strip() # Removes double or more spaces\n",
    "    return txt\n",
    "    \n",
    "df['tweet_text'] = df['tweet_text'].progress_map(process_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcula um embedding para tweets usando os embeddings aprendidos do dataset do capes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_embedding(text):\n",
    "    emb = np.zeros(dim)\n",
    "    \n",
    "    for word in text.split(' '):\n",
    "        emb = emb + get_embedding(word)\n",
    "    return emb / len(text.split(' '))\n",
    "    \n",
    "X = np.array([tweet_embedding(txt) for txt in tqdm(df['tweet_text'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a variável-resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define o split de treino e teste, e fita um modelo de regressão logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression().fit(x_train, y_train, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.predict(x_test) == y_test).mean()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
